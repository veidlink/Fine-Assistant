{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veidlink/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import clickhouse_connect\n",
    "import requests\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import clickhouse_connect\n",
    "import requests\n",
    "import json\n",
    "import requests\n",
    "import uuid\n",
    "from environs import Env \n",
    "from openai import OpenAI\n",
    "from embedding import E5LargeEmbeddingFunction\n",
    "from similarity import prep_query,_clickhouse_query_window\n",
    "from langchain_community.chat_models import GigaChat\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://3c761df8a0bd4f415a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3c761df8a0bd4f415a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Env()                # Создаем экземпляр класса Env\n",
    "env.read_env(path='.env')  # Методом read_env() читаем файл .env и загружаем из него переменные в окружение \n",
    "\n",
    "SBER_AUTH = env('SBER_AUTH')\n",
    "OPENAI_API_KEY = env('OPENAI_API_KEY')\n",
    "\n",
    "client = clickhouse_connect.get_client(host='fre1otkh1b.europe-west4.gcp.clickhouse.cloud', port=8443, username='default', password='.SAxpSMBh27IT')\n",
    "emb_func = E5LargeEmbeddingFunction()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "### SYSTEM ###\n",
    "Ты - дружелюбный правовой ассистент.\n",
    "Ты должен давать информативные ответы по сути вопроса только на основании предоставленного источика.\n",
    "Если вопрос не относится к теме права, ты должен отвечать \"Давай сменим тему.\"\n",
    "Указывай в ответе статьи на которые ссылаешься, если приведены их номера.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = \"\"\"\n",
    "### CONTEXT ###\n",
    "\n",
    "##### ИСТОЧНИК #####\n",
    "{page_text}\n",
    "\n",
    "### USER ###\n",
    "{QUESTION}\n",
    "\n",
    "### ASSISTANT ###\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def message_llm(system_prompt, user_input): # Функция для общения с Мистраль\n",
    "    '''Function to message (test) our LLM'''\n",
    "\n",
    "    try:\n",
    "        url = 'https://golden-oyster-possible.ngrok-free.app/v1/chat/completions'  \n",
    "        \n",
    "\n",
    "        data = {\n",
    "          \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": system_prompt},\n",
    "            { \"role\": \"user\", \"content\": user_input}\n",
    "          ],\n",
    "          \"temperature\": 0.6,\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return eval(response.text.replace(\"\\n  \", \"\"))['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(\"Ошибка запроса:\\n\", response.status_code, response.text)\n",
    "            return \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}!\")\n",
    "        \n",
    "def format_output(answer, page_num):        # Функция для форматирования ответов моделей\n",
    "    output = f\"{answer}\"\n",
    "    if not 'сменим тему' in answer:\n",
    "        output += f\"\\n\\nИспользованные страницы КоАП РФ: {', '.join(map(str, set(page_num)))}\"\n",
    "    return output\n",
    "\n",
    "def get_gigatoken(auth_token, scope='GIGACHAT_API_PERS'):   # Функция для получения токена GigaChat\n",
    "    \"\"\"\n",
    "      Выполняет POST-запрос к эндпоинту, который выдает токен.\n",
    "\n",
    "      Параметры:\n",
    "      - auth_token (str): токен авторизации, необходимый для запроса.\n",
    "      - область (str): область действия запроса API. По умолчанию — «GIGACHAT_API_PERS».\n",
    "\n",
    "      Возвращает:\n",
    "      - ответ API, где токен и срок его \"годности\".\n",
    "      \"\"\"\n",
    "    \n",
    "    # Создадим идентификатор UUID (36 знаков)\n",
    "    rq_uid = str(uuid.uuid4())\n",
    "\n",
    "    # API URL\n",
    "    url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
    "\n",
    "    # Заголовки\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'Accept': 'application/json',\n",
    "        'RqUID': rq_uid,\n",
    "        'Authorization': f'Basic {auth_token}'\n",
    "    }\n",
    "\n",
    "    # Тело запроса\n",
    "    payload = {\n",
    "        'scope': scope\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Делаем POST запрос с отключенной SSL верификацией\n",
    "        response = requests.post(url, headers=headers, data=payload, verify=False)\n",
    "        return response\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Ошибка при получении токена для GigaChat: {str(e)}\")\n",
    "        return -1\n",
    "\n",
    "response = get_gigatoken(SBER_AUTH)                         # Получение токена GigaChat\n",
    "if response != 1:\n",
    "  giga_token = response.json()['access_token']\n",
    "\n",
    "# Инициализация моделей\n",
    "gigachat = GigaChat(access_token=giga_token, verify_ssl_certs=False)    \n",
    "chat_gpt = OpenAI(api_key=OPENAI_API_KEY)\n",
    "model = 'gpt-3.5-turbo'\n",
    "\n",
    "def ask_chatbot(mode, question):                            # Логика взаимодействия с 3 моделями, обернутая в функцию\n",
    "    gc.collect()\n",
    "    try:\n",
    "        formatted_rag_prompt = \"\"\n",
    "        query = prep_query(question)\n",
    "        result = _clickhouse_query_window(query=query, client=client, emb_func=emb_func, limit_knn=1)\n",
    "\n",
    "        page_text = ''\n",
    "        page_num = []\n",
    "\n",
    "        for source in result:    \n",
    "            page_num.append(source[0])\n",
    "            page_text += source[1]\n",
    "        \n",
    "        formatted_rag_prompt = rag_prompt.format(\n",
    "            page_text=page_text,\n",
    "            QUESTION=question\n",
    "        )\n",
    "\n",
    "        if mode == \"Mistral 7b\":\n",
    "            mis_answer = message_llm(system_prompt, formatted_rag_prompt)\n",
    "            return format_output(mis_answer, page_num)\n",
    "            \n",
    "        elif mode == \"ChatGPT\":\n",
    "            try:\n",
    "                completion = chat_gpt.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": formatted_rag_prompt}\n",
    "                    ],\n",
    "                    temperature=0.6,   # [0, 2] Higher values make output more random, lower values make it deterministic.\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "            return format_output(completion.choices[0].message.content, page_num)\n",
    "        \n",
    "\n",
    "        elif mode == \"GigaChat\":\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    content=system_prompt\n",
    "                ),\n",
    "                HumanMessage(\n",
    "                    content=formatted_rag_prompt)\n",
    "                ]\n",
    "\n",
    "            giga_answer = gigachat.invoke(input=messages)\n",
    "\n",
    "            return format_output(giga_answer.content, page_num)\n",
    "    except Exception as e:\n",
    "        return f\"Ошибка. Попробуйте еще раз.\"  \n",
    "\n",
    "\n",
    "iface = gr.Interface(                                       # GUI Gradio\n",
    "    fn=ask_chatbot,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=[\"Mistral 7b\", \"ChatGPT\", \"GigaChat\"], label=\"Выберите режим\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Введите ваш вопрос здесь...\", label=\"Вопрос\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Ответ\"),\n",
    "    title=\"LLM-based RAG Fine Assistant\",\n",
    "    description=\"Выберите режим и введите ваше сообщение, чтобы получить ответ от модели:\"\n",
    ")\n",
    "\n",
    "# Запуск интерфейса\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
